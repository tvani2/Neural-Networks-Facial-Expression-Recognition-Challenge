{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3XswuWFEj0WcBkUg9QfYP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tvani2/Neural-Networks-Facial-Expression-Recognition-Challenge/blob/main/Expr_recognition_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9KhRoEkcIJr",
        "outputId": "ba1c95bc-40ff-4f0b-c1f1-1e0277841693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install kaggle wandb onnx -Uq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hjFEFAXmczRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c16d54d3-25f3-427a-b80e-973aef94b526"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/cs231n/assignments/assignment4/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "! unzip challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "metadata": {
        "id": "wvO1xmmdc0bN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "713f881d-6210-4474-9951-a7f387f11c37"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /content\n",
            " 87% 248M/285M [00:00<00:00, 853MB/s] \n",
            "100% 285M/285M [00:00<00:00, 864MB/s]\n",
            "Archive:  challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "  inflating: example_submission.csv  \n",
            "  inflating: fer2013.tar.gz          \n",
            "  inflating: icml_face_data.csv      \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "sPgxm7PTc2gd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5630b3bd-7b63-475b-cd9f-47098c4793cc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "# Change the file path to the correct location after unzipping\n",
        "df = pd.read_csv('./train.csv')\n",
        "X = df['pixels']\n",
        "y = df['emotion']\n",
        "\n",
        "train_size = 0.70\n",
        "val_size = 0.15\n",
        "test_size = 0.15\n",
        "X_temp, X_test_new, y_temp, y_test_new = train_test_split(\n",
        "    X, y, test_size=test_size, random_state=42, stratify=y\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=(val_size / (train_size + val_size)), random_state=42, stratify=y_temp\n",
        ")"
      ],
      "metadata": {
        "id": "izij90cffH3V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "def fast_process_pixels(pixel_series):\n",
        "    pixel_lists = pixel_series.str.split()\n",
        "    pixel_array = np.array(pixel_lists.tolist(), dtype=np.float32)\n",
        "    return pixel_array.reshape(-1, 48, 48, 1) / 255.0\n",
        "\n",
        "X_train_normalized = fast_process_pixels(X_train)\n",
        "X_val_normalized = fast_process_pixels(X_val)\n",
        "X_test_new_normalized = fast_process_pixels(X_test_new)\n",
        "\n",
        "print(\"Data preprocessing completed!\")\n",
        "print(f\"Train shape: {X_train_normalized.shape}\")\n",
        "print(f\"Val shape: {X_val_normalized.shape}\")\n",
        "print(f\"Test shape: {X_test_new_normalized.shape}\")\n",
        "\n",
        "# === 4. Dataset Class ===\n",
        "class FastEmotionDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = torch.from_numpy(images).permute(0, 3, 1, 2).float()\n",
        "        self.labels = torch.from_numpy(labels.values).long()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "lAYAlHekfdLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "652810a7-ae08-49df-9975-636f6b870b31"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing completed!\n",
            "Train shape: (20095, 48, 48, 1)\n",
            "Val shape: (4307, 48, 48, 1)\n",
            "Test shape: (4307, 48, 48, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
        "\n",
        "# === 5. Transforms ===\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# === 6. Create Datasets ===\n",
        "train_dataset = FastEmotionDataset(X_train_normalized, y_train, transform=train_transforms)\n",
        "val_dataset = FastEmotionDataset(X_val_normalized, y_val, transform=val_test_transforms)\n",
        "test_dataset = FastEmotionDataset(X_test_new_normalized, y_test_new, transform=val_test_transforms)\n",
        "\n",
        "# === 7. Compute Class Weights and Sampler ===\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "sample_weights = np.array([class_weights_dict[label] for label in y_train])\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# === 8. DataLoaders ===\n",
        "batch_size = 64\n",
        "num_workers = 0\n",
        "pin_memory = torch.cuda.is_available()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory,\n",
        "                          persistent_workers=num_workers > 0)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                        num_workers=num_workers, pin_memory=pin_memory,\n",
        "                        persistent_workers=num_workers > 0)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                         num_workers=num_workers, pin_memory=pin_memory,\n",
        "                         persistent_workers=num_workers > 0)"
      ],
      "metadata": {
        "id": "FhJ7KbmOfgX5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        # Convert to torch tensors and rearrange dimensions (H, W, C) -> (C, H, W)\n",
        "        self.images = torch.FloatTensor(images).permute(0, 3, 1, 2)\n",
        "        self.labels = torch.LongTensor(labels.values)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define data augmentation transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# No augmentation for validation and test\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Same normalization as training\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = EmotionDataset(X_train_normalized, y_train, transform=train_transforms)\n",
        "val_dataset = EmotionDataset(X_val_normalized, y_val, transform=val_test_transforms)\n",
        "test_dataset = EmotionDataset(X_test_new_normalized, y_test_new, transform=val_test_transforms)"
      ],
      "metadata": {
        "id": "GydKsAa6fu2-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# Create sample weights for each training sample\n",
        "sample_weights = [class_weights_dict[label] for label in y_train]\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")"
      ],
      "metadata": {
        "id": "y9DIR_Vtg3vG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "batch_size = 64\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_new_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Print class distribution info\n",
        "print(\"Class distribution in training set:\")\n",
        "print(y_train.value_counts().sort_index())\n",
        "print(f\"\\nClass weights: {class_weights_dict}\")"
      ],
      "metadata": {
        "id": "Yg6JM8Zbg5PG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ee288f-fc75-4218-8cf7-a6b08b5e8f8a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in training set:\n",
            "emotion\n",
            "0    2797\n",
            "1     306\n",
            "2    2867\n",
            "3    5051\n",
            "4    3380\n",
            "5    2219\n",
            "6    3475\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class weights: {0: np.float64(1.0263547678635272), 1: np.float64(9.381419234360411), 2: np.float64(1.0012955304200508), 3: np.float64(0.568345730689821), 4: np.float64(0.849323753169907), 5: np.float64(1.2936972896414085), 6: np.float64(0.8261048304213772)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "class EmotionResNet(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(EmotionResNet, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)"
      ],
      "metadata": {
        "id": "4h-kKE2s2Tg0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, min_delta=0.001, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            if self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            if self.restore_best_weights and self.best_weights:\n",
        "                model.load_state_dict(self.best_weights)\n",
        "            return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "whYJjukR_pW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"Facial_Expression_Recognition_1\", name=\"ResNet_1\")\n",
        "\n",
        "wandb.config.update({\n",
        "    \"model\": \"ResNet18\",\n",
        "    \"epochs\": 20,\n",
        "    \"batch_size\": train_loader.batch_size,\n",
        "    \"learning_rate\": 0.0003,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"scheduler\": \"ReduceLROnPlateau\",\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"early_stopping_patience\": 7\n",
        "})\n",
        "raw_data_artifact = wandb.Artifact(\n",
        "    name=\"facial-expression-dataset\",\n",
        "    type=\"dataset\",\n",
        "    description=\"Facial Expression Recognition Challenge Dataset\"\n",
        ")\n",
        "# Log the artifact to the current run\n",
        "wandb.log_artifact(raw_data_artifact)\n",
        "\n",
        "print(\"Data loaded and logged as a wandb artifact.\")"
      ],
      "metadata": {
        "id": "60Qr3afFA5GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import wandb\n",
        "import numpy as np\n",
        "\n",
        "# Define model, optimizer, scheduler\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = EmotionResNet(num_classes=7).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
        "early_stopping = EarlyStopping(patience=7, min_delta=0.0005)\n",
        "\n",
        "wandb.watch(model, log=\"all\", log_freq=100)\n",
        "\n",
        "train_losses, train_accuracies = [], []\n",
        "val_losses, val_accuracies = [], []\n",
        "model_save_path = \"best_resnet_model.pth\"\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_acc = 100. * correct / total\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss_total, correct_val, total_val = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss_total += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss_total / len(val_loader.dataset)\n",
        "    val_acc = 100. * correct_val / total_val\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}: Train Loss = {avg_train_loss:.4f}, Train Acc = {train_acc:.2f}%, \"\n",
        "          f\"Val Loss = {avg_val_loss:.4f}, Val Acc = {val_acc:.2f}%\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": avg_train_loss,\n",
        "        \"train_accuracy\": train_acc,\n",
        "        \"val_loss\": avg_val_loss,\n",
        "        \"val_accuracy\": val_acc,\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    if early_stopping(avg_val_loss, model):\n",
        "        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "# Load best model for inference/testing\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "pt8a2M_a7J5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(model_save_path))\n",
        "\n",
        "# Final Evaluation\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "# Ensure device is defined\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device) # Ensure model is on the correct device for evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score # Import accuracy_score\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Move images and labels to the same device as the model\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_accuracy = accuracy_score(all_labels, all_preds) * 100\n",
        "print(f\"\\n✅ Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "NGf1jjV1Zu5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class EmotionResNet(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(EmotionResNet, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Replace first conv layer for grayscale input (1 channel)\n",
        "        self.resnet.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),  # Added batch norm\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Replace FC with dropout and final classification layer\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)"
      ],
      "metadata": {
        "id": "qgEvK1Dzd_OO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import wandb\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EmotionResNet(num_classes=7).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
        "early_stopping = EarlyStopping(patience=7, min_delta=0.0005)\n",
        "\n",
        "wandb.init(project=\"Facial_Expression_Recognition_1\")\n",
        "wandb.watch(model, log=\"all\", log_freq=100)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "model_save_path = \"best_resnet_model.pth\"\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_acc = 100. * correct / total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss_total, correct_val, total_val = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss_total += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss_total / len(val_loader.dataset)\n",
        "    val_acc = 100. * correct_val / total_val\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}: Train Loss = {avg_train_loss:.4f}, Train Acc = {train_acc:.2f}%, \"\n",
        "          f\"Val Loss = {avg_val_loss:.4f}, Val Acc = {val_acc:.2f}%\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": avg_train_loss,\n",
        "        \"train_accuracy\": train_acc,\n",
        "        \"val_loss\": avg_val_loss,\n",
        "        \"val_accuracy\": val_acc,\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    if early_stopping(avg_val_loss, model):\n",
        "        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iLZ1I_7-eCZV",
        "outputId": "a0a29980-75b6-4303-ef01-858e09ad9c85"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rosy-voice-16</strong> at: <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/b520vsgk' target=\"_blank\">https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/b520vsgk</a><br> View project at: <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1' target=\"_blank\">https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250605_154830-b520vsgk/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250605_155020-ids248hh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/ids248hh' target=\"_blank\">smooth-durian-17</a></strong> to <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1' target=\"_blank\">https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/ids248hh' target=\"_blank\">https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/ids248hh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01: Train Loss = 1.9746, Train Acc = 25.24%, Val Loss = 1.6634, Val Acc = 35.87%\n",
            "Epoch 02: Train Loss = 1.5445, Train Acc = 41.25%, Val Loss = 1.6258, Val Acc = 41.54%\n",
            "Epoch 03: Train Loss = 1.3538, Train Acc = 49.29%, Val Loss = 1.3776, Val Acc = 48.08%\n",
            "Epoch 04: Train Loss = 1.2512, Train Acc = 53.25%, Val Loss = 1.3585, Val Acc = 48.66%\n",
            "Epoch 05: Train Loss = 1.1756, Train Acc = 55.81%, Val Loss = 1.2298, Val Acc = 53.05%\n",
            "Epoch 06: Train Loss = 1.1076, Train Acc = 58.23%, Val Loss = 1.2087, Val Acc = 54.10%\n",
            "Epoch 07: Train Loss = 1.0698, Train Acc = 59.67%, Val Loss = 1.2758, Val Acc = 53.84%\n",
            "Epoch 08: Train Loss = 1.0136, Train Acc = 62.21%, Val Loss = 1.1937, Val Acc = 55.44%\n",
            "Epoch 09: Train Loss = 0.9756, Train Acc = 63.60%, Val Loss = 1.2234, Val Acc = 55.28%\n",
            "Epoch 10: Train Loss = 0.9414, Train Acc = 65.17%, Val Loss = 1.2016, Val Acc = 56.14%\n",
            "Epoch 11: Train Loss = 0.9017, Train Acc = 66.76%, Val Loss = 1.1801, Val Acc = 56.70%\n",
            "Epoch 12: Train Loss = 0.8631, Train Acc = 68.15%, Val Loss = 1.2000, Val Acc = 56.56%\n",
            "Epoch 13: Train Loss = 0.8441, Train Acc = 69.19%, Val Loss = 1.2324, Val Acc = 55.26%\n",
            "Epoch 14: Train Loss = 0.8062, Train Acc = 70.66%, Val Loss = 1.2212, Val Acc = 55.77%\n",
            "Epoch 15: Train Loss = 0.7779, Train Acc = 71.63%, Val Loss = 1.2269, Val Acc = 57.19%\n",
            "Epoch 16: Train Loss = 0.6744, Train Acc = 75.31%, Val Loss = 1.2352, Val Acc = 59.00%\n",
            "Epoch 17: Train Loss = 0.6335, Train Acc = 77.16%, Val Loss = 1.2905, Val Acc = 58.65%\n",
            "Epoch 18: Train Loss = 0.5775, Train Acc = 79.20%, Val Loss = 1.2795, Val Acc = 59.90%\n",
            "Early stopping at epoch 18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>learning_rate</td><td>███████████████▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▅▅▅▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▅▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▇▄▄▂▁▂▁▂▁▁▁▂▂▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>18</td></tr><tr><td>learning_rate</td><td>0.00015</td></tr><tr><td>train_accuracy</td><td>79.19881</td></tr><tr><td>train_loss</td><td>0.5775</td></tr><tr><td>val_accuracy</td><td>59.90248</td></tr><tr><td>val_loss</td><td>1.27951</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">smooth-durian-17</strong> at: <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/ids248hh' target=\"_blank\">https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/ids248hh</a><br> View project at: <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1' target=\"_blank\">https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250605_155020-ids248hh/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ImprovedEmotionResNet(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_rate=0.5):\n",
        "        super(ImprovedEmotionResNet, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Replace first conv layer for grayscale input (1 channel)\n",
        "        self.resnet.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.add_dropout_to_layers()\n",
        "\n",
        "        # Enhanced classifier with multiple dropout layers\n",
        "        num_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(num_features, num_features // 2),\n",
        "            nn.BatchNorm1d(num_features // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate * 0.7),  # Slightly less dropout in second layer\n",
        "            nn.Linear(num_features // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def add_dropout_to_layers(self):\n",
        "        \"\"\"Add dropout after each residual block\"\"\"\n",
        "        # Add dropout after layer1\n",
        "        original_layer1 = self.resnet.layer1\n",
        "        self.resnet.layer1 = nn.Sequential(\n",
        "            original_layer1,\n",
        "            nn.Dropout2d(0.1)  # Light spatial dropout\n",
        "        )\n",
        "\n",
        "        # Add dropout after layer2\n",
        "        original_layer2 = self.resnet.layer2\n",
        "        self.resnet.layer2 = nn.Sequential(\n",
        "            original_layer2,\n",
        "            nn.Dropout2d(0.15)\n",
        "        )\n",
        "\n",
        "        # Add dropout after layer3\n",
        "        original_layer3 = self.resnet.layer3\n",
        "        self.resnet.layer3 = nn.Sequential(\n",
        "            original_layer3,\n",
        "            nn.Dropout2d(0.2)\n",
        "        )\n",
        "\n",
        "        # Add dropout after layer4\n",
        "        original_layer4 = self.resnet.layer4\n",
        "        self.resnet.layer4 = nn.Sequential(\n",
        "            original_layer4,\n",
        "            nn.Dropout2d(0.25)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)"
      ],
      "metadata": {
        "id": "FUPEUAlF3qzx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project=\"Facial_Expression_Recognition_1\",\n",
        "    config={\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"architecture\": \"ImprovedEmotionResNet\",\n",
        "        \"dataset\": \"FER2013\",\n",
        "        \"epochs\": 30,\n",
        "        \"batch_size\": 32,\n",
        "        \"optimizer\": \"AdamW\",\n",
        "        \"weight_decay\": 1e-3,\n",
        "        \"label_smoothing\": 0.1,\n",
        "        \"dropout_rate\": 0.5\n",
        "    }\n",
        ")\n",
        "wandb.watch(model, log=\"all\", log_freq=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "zFEblOdo3t8U",
        "outputId": "ff80656e-1e55-4442-c615-2c17bc45ca51"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250605_162245-5lp60jjv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/5lp60jjv' target=\"_blank\">crimson-jazz-18</a></strong> to <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1' target=\"_blank\">https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/5lp60jjv' target=\"_blank\">https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/5lp60jjv</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import wandb\n",
        "import numpy as np\n",
        "\n",
        "# Define model, optimizer, scheduler\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ImprovedEmotionResNet(num_classes=7, dropout_rate=0.5).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0002, weight_decay=1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
        "early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
        "\n",
        "wandb.watch(model, log=\"all\", log_freq=100)\n",
        "\n",
        "train_losses, train_accuracies = [], []\n",
        "val_losses, val_accuracies = [], []\n",
        "model_save_path = \"best_resnet_model.pth\"\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_acc = 100. * correct / total\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss_total, correct_val, total_val = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss_total += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss_total / len(val_loader.dataset)\n",
        "    val_acc = 100. * correct_val / total_val\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}: Train Loss = {avg_train_loss:.4f}, Train Acc = {train_acc:.2f}%, \"\n",
        "          f\"Val Loss = {avg_val_loss:.4f}, Val Acc = {val_acc:.2f}%\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": avg_train_loss,\n",
        "        \"train_accuracy\": train_acc,\n",
        "        \"val_loss\": avg_val_loss,\n",
        "        \"val_accuracy\": val_acc,\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    if early_stopping(avg_val_loss, model):\n",
        "        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "# Load best model for inference/testing\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "\n",
        "# Final Evaluation\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "# Ensure device is defined\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device) # Ensure model is on the correct device for evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Move images and labels to the same device as the model\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_accuracy = accuracy_score(all_labels, all_preds) * 100\n",
        "print(f\"\\n✅ Test Accuracy: {test_accuracy:.2f}%\")\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "id": "kM1eEAwf5wXR",
        "outputId": "44cd2178-83a0-4b00-d17f-045172d1cd1a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01: Train Loss = 1.9678, Train Acc = 17.75%, Val Loss = 1.8298, Val Acc = 27.44%\n",
            "Epoch 02: Train Loss = 1.7794, Train Acc = 29.30%, Val Loss = 1.5854, Val Acc = 39.03%\n",
            "Epoch 03: Train Loss = 1.6065, Train Acc = 38.86%, Val Loss = 1.5380, Val Acc = 40.79%\n",
            "Epoch 04: Train Loss = 1.5062, Train Acc = 43.09%, Val Loss = 1.4401, Val Acc = 45.07%\n",
            "Epoch 05: Train Loss = 1.4321, Train Acc = 46.38%, Val Loss = 1.4056, Val Acc = 46.58%\n",
            "Epoch 06: Train Loss = 1.3856, Train Acc = 48.28%, Val Loss = 1.3322, Val Acc = 49.71%\n",
            "Epoch 07: Train Loss = 1.3278, Train Acc = 50.92%, Val Loss = 1.3272, Val Acc = 50.20%\n",
            "Epoch 08: Train Loss = 1.2757, Train Acc = 53.01%, Val Loss = 1.2702, Val Acc = 51.96%\n",
            "Epoch 09: Train Loss = 1.2641, Train Acc = 53.15%, Val Loss = 1.2988, Val Acc = 51.52%\n",
            "Epoch 10: Train Loss = 1.2310, Train Acc = 54.38%, Val Loss = 1.3092, Val Acc = 51.71%\n",
            "Epoch 11: Train Loss = 1.2104, Train Acc = 55.32%, Val Loss = 1.2772, Val Acc = 52.82%\n",
            "Epoch 12: Train Loss = 1.1839, Train Acc = 56.73%, Val Loss = 1.2545, Val Acc = 52.31%\n",
            "Epoch 13: Train Loss = 1.1696, Train Acc = 56.84%, Val Loss = 1.2238, Val Acc = 53.77%\n",
            "Epoch 14: Train Loss = 1.1592, Train Acc = 57.74%, Val Loss = 1.1950, Val Acc = 55.61%\n",
            "Epoch 15: Train Loss = 1.1476, Train Acc = 58.04%, Val Loss = 1.1741, Val Acc = 56.26%\n",
            "Epoch 16: Train Loss = 1.1283, Train Acc = 58.54%, Val Loss = 1.1761, Val Acc = 56.07%\n",
            "Epoch 17: Train Loss = 1.1084, Train Acc = 59.83%, Val Loss = 1.1832, Val Acc = 55.31%\n",
            "Epoch 18: Train Loss = 1.0950, Train Acc = 60.42%, Val Loss = 1.2400, Val Acc = 54.17%\n",
            "Epoch 19: Train Loss = 1.0998, Train Acc = 59.66%, Val Loss = 1.2314, Val Acc = 54.82%\n",
            "Epoch 20: Train Loss = 1.0291, Train Acc = 62.53%, Val Loss = 1.1521, Val Acc = 57.14%\n",
            "\n",
            "Evaluating on test set...\n",
            "\n",
            "✅ Test Accuracy: 58.16%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>learning_rate</td><td>███████████████████▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▅▆▆▇▇▇▇▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▇▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▂▁</td></tr><tr><td>val_accuracy</td><td>▁▄▄▅▆▆▆▇▇▇▇▇▇████▇▇█</td></tr><tr><td>val_loss</td><td>█▅▅▄▄▃▃▂▃▃▂▂▂▁▁▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train_accuracy</td><td>62.52799</td></tr><tr><td>train_loss</td><td>1.02911</td></tr><tr><td>val_accuracy</td><td>57.13954</td></tr><tr><td>val_loss</td><td>1.15205</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">crimson-jazz-18</strong> at: <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/5lp60jjv' target=\"_blank\">https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/5lp60jjv</a><br> View project at: <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1' target=\"_blank\">https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250605_162245-5lp60jjv/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}