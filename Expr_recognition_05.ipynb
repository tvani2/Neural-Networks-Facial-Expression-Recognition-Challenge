{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMes+ekIifjJUBiBiG4rhts",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tvani2/Neural-Networks-Facial-Expression-Recognition-Challenge/blob/main/Expr_recognition_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAZ0FYWl0dV2",
        "outputId": "2b4991e8-3425-465a-eaef-2216682618bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install kaggle wandb onnx -Uq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A74BG9e10fon",
        "outputId": "aa2f9210-26a7-4cab-a4fb-d104e51b6a84"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/cs231n/assignments/assignment4/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "! unzip challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxZPPXrJ0glQ",
        "outputId": "66e832cf-e523-4a5d-e47c-33c93ba33456"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /content\n",
            " 88% 250M/285M [00:00<00:00, 637MB/s] \n",
            "100% 285M/285M [00:02<00:00, 121MB/s]\n",
            "Archive:  challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "  inflating: example_submission.csv  \n",
            "  inflating: fer2013.tar.gz          \n",
            "  inflating: icml_face_data.csv      \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "7UUD_K5f0idf",
        "outputId": "91a2dc39-4857-4a4b-99f4-4b6bc48511ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtvani22\u001b[0m (\u001b[33mtvani22-free-university-of-tbilisi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "# Change the file path to the correct location after unzipping\n",
        "df = pd.read_csv('./train.csv')\n",
        "X = df['pixels']\n",
        "y = df['emotion']\n",
        "\n",
        "train_size = 0.70\n",
        "val_size = 0.15\n",
        "test_size = 0.15\n",
        "X_temp, X_test_new, y_temp, y_test_new = train_test_split(\n",
        "    X, y, test_size=test_size, random_state=42, stratify=y\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=(val_size / (train_size + val_size)), random_state=42, stratify=y_temp\n",
        ")"
      ],
      "metadata": {
        "id": "WxkUatoi0jwC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "def fast_process_pixels(pixel_series):\n",
        "    pixel_lists = pixel_series.str.split()\n",
        "    pixel_array = np.array(pixel_lists.tolist(), dtype=np.float32)\n",
        "    return pixel_array.reshape(-1, 48, 48, 1) / 255.0\n",
        "\n",
        "X_train_normalized = fast_process_pixels(X_train)\n",
        "X_val_normalized = fast_process_pixels(X_val)\n",
        "X_test_new_normalized = fast_process_pixels(X_test_new)\n",
        "\n",
        "print(\"Data preprocessing completed!\")\n",
        "print(f\"Train shape: {X_train_normalized.shape}\")\n",
        "print(f\"Val shape: {X_val_normalized.shape}\")\n",
        "print(f\"Test shape: {X_test_new_normalized.shape}\")\n",
        "\n",
        "# === 4. Dataset Class ===\n",
        "class FastEmotionDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = torch.from_numpy(images).permute(0, 3, 1, 2).float()\n",
        "        self.labels = torch.from_numpy(labels.values).long()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UzX7lKi0loU",
        "outputId": "8deccecc-2b39-4604-e0d0-7fc589f63379"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing completed!\n",
            "Train shape: (20095, 48, 48, 1)\n",
            "Val shape: (4307, 48, 48, 1)\n",
            "Test shape: (4307, 48, 48, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
        "\n",
        "# === 5. Transforms ===\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# === 6. Create Datasets ===\n",
        "train_dataset = FastEmotionDataset(X_train_normalized, y_train, transform=train_transforms)\n",
        "val_dataset = FastEmotionDataset(X_val_normalized, y_val, transform=val_test_transforms)\n",
        "test_dataset = FastEmotionDataset(X_test_new_normalized, y_test_new, transform=val_test_transforms)\n",
        "\n",
        "# === 7. Compute Class Weights and Sampler ===\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "sample_weights = np.array([class_weights_dict[label] for label in y_train])\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# === 8. DataLoaders ===\n",
        "batch_size = 64\n",
        "num_workers = 0\n",
        "pin_memory = torch.cuda.is_available()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory,\n",
        "                          persistent_workers=num_workers > 0)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                        num_workers=num_workers, pin_memory=pin_memory,\n",
        "                        persistent_workers=num_workers > 0)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                         num_workers=num_workers, pin_memory=pin_memory,\n",
        "                         persistent_workers=num_workers > 0)"
      ],
      "metadata": {
        "id": "sbfBAYkF0l_l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        # Convert to torch tensors and rearrange dimensions (H, W, C) -> (C, H, W)\n",
        "        self.images = torch.FloatTensor(images).permute(0, 3, 1, 2)\n",
        "        self.labels = torch.LongTensor(labels.values)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define data augmentation transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# No augmentation for validation and test\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Same normalization as training\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = EmotionDataset(X_train_normalized, y_train, transform=train_transforms)\n",
        "val_dataset = EmotionDataset(X_val_normalized, y_val, transform=val_test_transforms)\n",
        "test_dataset = EmotionDataset(X_test_new_normalized, y_test_new, transform=val_test_transforms)"
      ],
      "metadata": {
        "id": "NfBVnlrY0odG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# Create sample weights for each training sample\n",
        "sample_weights = [class_weights_dict[label] for label in y_train]\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")"
      ],
      "metadata": {
        "id": "YRaLcV_d0qOZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "batch_size = 64\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_new_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Print class distribution info\n",
        "print(\"Class distribution in training set:\")\n",
        "print(y_train.value_counts().sort_index())\n",
        "print(f\"\\nClass weights: {class_weights_dict}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUkODC240swZ",
        "outputId": "50704d95-fccc-4c16-d16c-4e3bf09be0b6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in training set:\n",
            "emotion\n",
            "0    2797\n",
            "1     306\n",
            "2    2867\n",
            "3    5051\n",
            "4    3380\n",
            "5    2219\n",
            "6    3475\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class weights: {0: np.float64(1.0263547678635272), 1: np.float64(9.381419234360411), 2: np.float64(1.0012955304200508), 3: np.float64(0.568345730689821), 4: np.float64(0.849323753169907), 5: np.float64(1.2936972896414085), 6: np.float64(0.8261048304213772)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class ImprovedEmotionResNet(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_rate=0.5):\n",
        "        super(ImprovedEmotionResNet, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Replace the first layer for grayscale input\n",
        "        self.resnet.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self._add_dropout_layers()\n",
        "\n",
        "        num_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(num_features, num_features // 2),\n",
        "            nn.BatchNorm1d(num_features // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate * 0.7),\n",
        "            nn.Linear(num_features // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def _add_dropout_layers(self):\n",
        "        self.resnet.layer1 = nn.Sequential(self.resnet.layer1, nn.Dropout2d(0.1))\n",
        "        self.resnet.layer2 = nn.Sequential(self.resnet.layer2, nn.Dropout2d(0.15))\n",
        "        self.resnet.layer3 = nn.Sequential(self.resnet.layer3, nn.Dropout2d(0.2))\n",
        "        self.resnet.layer4 = nn.Sequential(self.resnet.layer4, nn.Dropout2d(0.25))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)"
      ],
      "metadata": {
        "id": "wbApzZj7Jb9M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "\n",
        "    def __call__(self, val_loss, model=None):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "        return self.counter >= self.patience"
      ],
      "metadata": {
        "id": "G94cLbYHJ1wE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import wandb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ImprovedEmotionResNet(num_classes=7, dropout_rate=0.5).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0002, weight_decay=1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
        "early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
        "\n",
        "wandb.init(project=\"Facial_Expression_Recognition_1\")\n",
        "wandb.watch(model, log=\"all\", log_freq=100)\n",
        "\n",
        "model_save_path = \"best_resnet_model.pth\"\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_acc = 100. * correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss_total, correct_val, total_val = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss_total += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss_total / len(val_loader.dataset)\n",
        "    val_acc = 100. * correct_val / total_val\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}: Train Loss = {avg_train_loss:.4f}, Train Acc = {train_acc:.2f}%, \"\n",
        "          f\"Val Loss = {avg_val_loss:.4f}, Val Acc = {val_acc:.2f}%\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": avg_train_loss,\n",
        "        \"train_accuracy\": train_acc,\n",
        "        \"val_loss\": avg_val_loss,\n",
        "        \"val_accuracy\": val_acc,\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    if early_stopping(avg_val_loss):\n",
        "        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "model.eval()\n",
        "\n",
        "# Final Test Evaluation\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_accuracy = accuracy_score(all_labels, all_preds) * 100\n",
        "print(f\"\\n✅ Test Accuracy: {test_accuracy:.2f}%\")\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cP-dXFrzJuZh",
        "outputId": "0b0c1444-dbe4-4029-806b-8d53f86051cf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 210MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250605_174456-b4mw5uz7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/b4mw5uz7' target=\"_blank\">smooth-donkey-20</a></strong> to <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1' target=\"_blank\">https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/b4mw5uz7' target=\"_blank\">https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/b4mw5uz7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01: Train Loss = 1.9601, Train Acc = 18.15%, Val Loss = 1.8352, Val Acc = 24.66%\n",
            "Epoch 02: Train Loss = 1.7362, Train Acc = 32.21%, Val Loss = 1.6079, Val Acc = 36.89%\n",
            "Epoch 03: Train Loss = 1.5743, Train Acc = 39.73%, Val Loss = 1.4937, Val Acc = 42.54%\n",
            "Epoch 04: Train Loss = 1.4833, Train Acc = 43.81%, Val Loss = 1.3840, Val Acc = 47.20%\n",
            "Epoch 05: Train Loss = 1.4071, Train Acc = 47.23%, Val Loss = 1.3788, Val Acc = 46.92%\n",
            "Epoch 06: Train Loss = 1.3622, Train Acc = 49.26%, Val Loss = 1.3818, Val Acc = 47.85%\n",
            "Epoch 07: Train Loss = 1.3074, Train Acc = 51.11%, Val Loss = 1.3060, Val Acc = 50.62%\n",
            "Epoch 08: Train Loss = 1.2789, Train Acc = 52.38%, Val Loss = 1.4617, Val Acc = 46.64%\n",
            "Epoch 09: Train Loss = 1.2498, Train Acc = 53.70%, Val Loss = 1.2464, Val Acc = 53.01%\n",
            "Epoch 10: Train Loss = 1.2220, Train Acc = 54.93%, Val Loss = 1.2520, Val Acc = 52.96%\n",
            "Epoch 11: Train Loss = 1.1997, Train Acc = 55.45%, Val Loss = 1.2364, Val Acc = 54.42%\n",
            "Epoch 12: Train Loss = 1.1751, Train Acc = 57.12%, Val Loss = 1.2177, Val Acc = 53.96%\n",
            "Epoch 13: Train Loss = 1.1652, Train Acc = 57.34%, Val Loss = 1.2069, Val Acc = 54.68%\n",
            "Epoch 14: Train Loss = 1.1565, Train Acc = 57.62%, Val Loss = 1.2101, Val Acc = 55.21%\n",
            "Epoch 15: Train Loss = 1.1381, Train Acc = 58.35%, Val Loss = 1.2165, Val Acc = 53.98%\n",
            "Epoch 16: Train Loss = 1.1138, Train Acc = 59.09%, Val Loss = 1.1664, Val Acc = 56.51%\n",
            "Epoch 17: Train Loss = 1.1123, Train Acc = 59.22%, Val Loss = 1.2012, Val Acc = 55.40%\n",
            "Epoch 18: Train Loss = 1.1061, Train Acc = 59.68%, Val Loss = 1.1605, Val Acc = 56.70%\n",
            "Epoch 19: Train Loss = 1.0918, Train Acc = 60.15%, Val Loss = 1.1687, Val Acc = 56.35%\n",
            "Epoch 20: Train Loss = 1.0858, Train Acc = 59.90%, Val Loss = 1.1711, Val Acc = 56.98%\n",
            "\n",
            "✅ Test Accuracy: 56.84%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▅▅▆▆▆▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▆▆▇▆▇▇▇▇██▇█████</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▃▃▄▂▂▂▂▁▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>learning_rate</td><td>0.0002</td></tr><tr><td>train_accuracy</td><td>59.90047</td></tr><tr><td>train_loss</td><td>1.0858</td></tr><tr><td>val_accuracy</td><td>56.97701</td></tr><tr><td>val_loss</td><td>1.17109</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">smooth-donkey-20</strong> at: <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/b4mw5uz7' target=\"_blank\">https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1/runs/b4mw5uz7</a><br> View project at: <a href='https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1' target=\"_blank\">https://wandb.ai/tvani22-free-university-of-tbilisi/Facial_Expression_Recognition_1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250605_174456-b4mw5uz7/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}